#include "_doctype.html"
<html>
<head> <title>hiper - schedule</title>
#include "css.t"
</head>

#define LIBCURL_HIPER
#define HIPER_SCHEDULE
#define CURL_URL libcurl/hiper/schedule.html

#include "_menu.html"
#include "setup.t"

WHERE3(libcurl, "/libcurl/", hiper, "/libcurl/hiper/", hiper schedule)

<a name=top>
TITLE(Project Hiper Schedule and Dates)
<div class="relatedbox">
<b>Related:</b>
<br><a href="roadmap.html">Roadmap</a>
</div>

#define MARKDATE(x)  <tr valign="top"><td nowrap><span class="whodate"> x </span></td><td>
#define STOP </td></tr>

<p>
 I'll try to add info here as I proceed. If you want to comment on anything
 here, or provide some other kind of feedback on the Hiper work, please post
 all such to the <a
 href="http://curl.haxx.se/mail/list.cgi?list=curl-library">curl-library
 mailing list</a>!

<table class="news">

MARKDATE(Dec 9 2005) I consider this round of tests to complete my first
 investigation and measure phase in which I've carefully studied how an
 applicaton performs with the existing API and the accompanying "enforced" use
 of select().

<p>
 I worked around the 1024 connection limits for both
 server and client side. I built the test program and ran it to use a single
 active connection and N number of idle ones (idle meaning the server never
 repsonds with any data, it just keeps the connection alive).
<p>
 The results were pretty much as I had imagined, and I'm pleased with the test
 setup and the stable and repeatable test numbers. Due to the amount of RAM in
 my test box, and the amount of memory required for each single connection in
 both the client and server (since they were running on the same box), I
 stopped testing at 9000 simultaneous connections since doing more would've
 forced use of swap and killed all reliable results.
<p>
 I ran the test client with 1001, 2001, 3001, 5001 and 9001 connections and
 measured how long select() and curl_multi_perform() would take in average,
 over a period of 20 seconds. Each read call in libcurl would only read one
 single byte, so thus we are sure that the active connection will be
 "readable" in the select() sense during the entire test. It also ensured that
 there would be no significant time spent in the read() call itself. The
 read() overhead is thus also constant in all the tests.
<p>
 <a href="perf-20051209.png"><img src="perf-20051209t.png" border="0"></a>
<p>
 The exact numbers of the graph are:
<pre>
Connections  multi_perform  select
1001         3504           951
2001         7606           1988
3001         11045          2715
5001         16406          4024
9001         32147          8030
</pre>
<p>
 Each time above is in microseconds and is the average time of 5 or 6 test runs.

<p>
 What do I hope for in new "socket" extension of the multi API, in pure
 numbers in comparison to graph above using a test that is identical "in
 spirit"? (using curl_multi_socket() instead of curl_multi_perform())
<p>
<ol>

<li> Time spent in curl_multi_socket() should be (more or less) fixed no
 matter how many idle connections that are used. I sincerely hope that means
 less than 10 microseconds in this test setup.

<li> The select() benchmark graph below suggests that libevent is pretty much
 fixed at 50 microseconds (although I don't know what test box was used in
 their testing, we can compare the select()-times from my tests and see that
 they are at least resonably close).

<li> Summing up, the collected ~40 ms spent at 9000 connections could possibly
 be lowered to something around 60 us!

</ol>

STOP

MARKDATE(Dec 7 2005) Different test/measure approach:

<p> I now have a local HTTP server that can serve two modes: idle and
active. The idle mode does nothing after it accepted the client's request,
while the active mode sends a never-ending stream.

<p>
The application now does X connects using idle mode and Y using active, and
then measure how long each call to select() and curl_multi_perform() takes
with a varying amount of idle connections added.

<p>
Of course I hit some minor quirks having to avoid the varios 1024 file
descriptor limits: the FD_SETSIZE default define is 1024 (limits how many
descriptors select() can use) and a user process defaults to allowing max 1024
file descritors. Working to smooth them out to be able to run tests using
1000, 2000 and 5000 idle connections with a single active one.

STOP

MARKDATE(Dec 6 2005) I committed my currently used code to CVS: <a
href="http://cool.haxx.se/cvs.cgi/curl/hiper/">cool.haxx.se/cvs.cgi/curl/hiper/</a>
- not terribly much to see but I wanted to added it to make it easier for me
to keep older versions around. This doesn't include the (minor) changes to
libcurl I've done so far but none of those changes are required for this to
run/work.

STOP

 MARKDATE(Dec 6 2005) Ok, I think I'll change my measuring approach and I will
 instead write up my own HTTP server and control that to allow it to either
 sit perfectly quiet (just keeping the connection alive) or to send an endless
 stream of data. By allowing the selection of behaviour be based on the URL, I
 should be able to modify my test program to run and test a given number of
 "active" connections with a given number of "existing connections".  I guess
 the real-world URLs are too hard to use to make adequate tests with.
<p>
 I found this little benchmark on select() performance compared to other event
 systems:
 <br>
 <a href="http://monkey.org/~provos/libevent/libevent-benchmark.jpg"><img border="0" src="bench.jpg" alt="graph comparing event systems' performances"></a>
<p>

STOP

 MARKDATE(Dec 6 2005) Been struggling to get a test program that can be used
 to measure and show the current overhead/speed in a good way. I've been quite
 surprised by the speed of the existing implementation. My test application
 easily initiates and transfers 2000 simultaneous transfers (and man does this
 work nicer with a c-ares built libcurl!). The average time spent for each
 curl_multi_perform() in these cases is easily measured, but it is not easily
 to draw any specific conclusions. With 50 simultaneous connections, the
 average time per invoke is less than 600us on my Athlon XP2800 Linux
 2.6/glibc 2.3.5/libcurl 7.15.1 test setup.
<p>
 Some more numbers from the tests. I have a list with a few thousand URLs that
 the test app reads from and starts transferring data from. See below for more
 descriptions.
<p>
<pre>
Total time 108149477us - Paused 12906983us = Active 95242494us = Active/total 47621us
 Active/(connections that delivered data) = 115585us
 824 out of 2000 connections provided data
46684 calls to curl_multi_perform(), average 206 alive. Average time: 2040us
45267 calls to select(), average 193 alive
 Average number of readable connections per select() return: 172
 Max number of readable connections for a single select() return: 549
0 select() timeouts
</pre>
<p>
<ul>
 <li> Total time - duration of the test. It stops when less than 50
 connections are alive
 <li> Paused - time we don't care about, during which select() is called
 <li> Active - time we care about. The total amount spent in curl_multi_perform()
 <li> Active/total - is then the active time split on total number of connections (2000 in this case)
 <li> It is worth noting that not during the test, not a single select() call
   timed out. I use a 50ms timeout
 <li> The numbers shown here are quoted from a single test run, but they are
   very similar on repeated invokes
</ul>
<p>
 I managed to get this very high number of simultaneous connections returned
 from the select() call by patching libcurl to only read from connections one
 byte at a time!
<p>
 The test program finally produces a summary that is separated into number of
 alive connections. So that we can see the average time curl_multi_perform()
 takes when N connections are still alive. I'll show a few lines from the list
 (times are in microseconds):

<pre>
Time 50 connections, average 579 max 2351 (283 laps) average/conn: 11
Time 60 connections, average 673 max 3036 (594 laps) average/conn: 11
Time 70 connections, average 877 max 5402 (566 laps) average/conn: 12
Time 100 connections, average 1134 max 2367 (431 laps) average/conn: 11
Time 150 connections, average 1787 max 3025 (11 laps) average/conn: 11
Time 200 connections, average 2112 max 3843 (74 laps) average/conn: 10
Time 300 connections, average 2901 max 5629 (27 laps) average/conn: 9
Time 401 connections, average 3855 max 8623 (65 laps) average/conn: 9
Time 500 connections, average 4552 max 8409 (25 laps) average/conn: 9
Time 915 connections, average 11700 max 38587 (6 laps) average/conn: 12
</pre>

<p>
 Worth noticing is that this splits the time on "alive connections", while we
 don't really know how many connections that are readable at any given time
 here. The average number shown above does indicate that the amount of
 readable connections are fairly high all the time though.

STOP

 MARKDATE(Dec 2 2005) Made a script that figuered out a few thousand "random"
 URLs.<p>Started writing a test application using the existing multi
 interface. It fetches N simultaneous URLs and measures how long time is spent
 in the curl_multi_perform() calls. With "just" 100 simultaneous downloads the
 time spent is very small so I guess I need to up the timer resolution and use
 many more simultaneous transfers. Since my goal is to increase performance
 when very many transfers <i>are in progress</i> I need to somehow weight the
 stats counters to consider the amount of current transfers.
 <p>
 I've decided I'll commit/shouw my test program(s) as soon as I have something
 that actually show numbers I trust and are reliable.

STOP

 MARKDATE(Dec 1 2005) Daniel started working full-time on project Hiper (during
 the entire month of December).
STOP

 MARKDATE(Nov 29 2005) Mailed the <a href="kvartalsrapport1.pdf">first
 quarterly report in Swedish</a> to IIS, about project Hiper. (282KB PDF)

STOP

 MARKDATE(Nov 28 2005) Discussing how the "zero copy" interface for
 libcurl could work. On the <a href="http://curl.haxx.se/mail/list.cgi?list=curl-library">curl-library</a> mailing list.

STOP

 MARKDATE(Nov 9 2005) Discussing possible Windows-adjustments to the hiper
 concept on the <a href="http://curl.haxx.se/mail/list.cgi?list=curl-library">curl-library</a> mailing list.
STOP

 MARKDATE(Nov 4 2005) Added the <a href="api.html">API</a> and <a
 href="app.html">Sample app</a> pages here.
STOP

 MARKDATE(Nov 3 2005) announced the 'hiper' project to the world, and added
 this hiper section of the curl web site.
STOP

 MARKDATE(Oct 24 2005) Daniel visited the "Internetdagarna" conference in
 Stockholm, Sweden and receives a diploma, a flower and some sturdy handshakes
 when the <a href="http://www.iis.se/">IIS</a> foundation announced that
 Daniel will receive 150,000 SEK from their fund. Daniel was notified about
 their decision roughly one week before this.
STOP


#include "_footer.html"

</BODY>
</HTML>
